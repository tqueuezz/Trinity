"""
Codebase Index Workflow
ä»£ç åº“ç´¢å¼•å·¥ä½œæµ

This workflow integrates the functionality of run_indexer.py and code_indexer.py
to build intelligent relationships between existing codebase and target structure.

è¯¥å·¥ä½œæµé›†æˆäº†run_indexer.pyå’Œcode_indexer.pyçš„åŠŸèƒ½ï¼Œ
ç”¨äºåœ¨ç°æœ‰ä»£ç åº“å’Œç›®æ ‡ç»“æ„ä¹‹é—´å»ºç«‹æ™ºèƒ½å…³ç³»ã€‚

Features:
- ä»initial_plan.txtæå–ç›®æ ‡æ–‡ä»¶ç»“æ„ / Extract target file structure from initial_plan.txt
- åˆ†æä»£ç åº“å¹¶å»ºç«‹ç´¢å¼• / Analyze codebase and build indexes
- ç”Ÿæˆå…³ç³»æ˜ å°„å’Œç»Ÿè®¡æŠ¥å‘Š / Generate relationship mappings and statistical reports
- ä¸ºä»£ç å¤ç°æä¾›å‚è€ƒä¾æ® / Provide reference basis for code reproduction
"""

import asyncio
import json
import logging
import os
import re
import sys
from pathlib import Path
from typing import Dict, Any, Optional
import yaml

# æ·»åŠ toolsç›®å½•åˆ°è·¯å¾„ä¸­ / Add tools directory to path
sys.path.append(str(Path(__file__).parent.parent / "tools"))

from tools.code_indexer import CodeIndexer


class CodebaseIndexWorkflow:
    """ä»£ç åº“ç´¢å¼•å·¥ä½œæµç±» / Codebase Index Workflow Class"""

    def __init__(self, logger=None):
        """
        åˆå§‹åŒ–å·¥ä½œæµ

        Args:
            logger: æ—¥å¿—è®°å½•å™¨å®ä¾‹
        """
        self.logger = logger or self._setup_default_logger()
        self.indexer = None

    def _setup_default_logger(self) -> logging.Logger:
        """è®¾ç½®é»˜è®¤æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("CodebaseIndexWorkflow")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def extract_file_tree_from_plan(self, plan_content: str) -> Optional[str]:
        """
        ä»initial_plan.txtå†…å®¹ä¸­æå–æ–‡ä»¶æ ‘ç»“æ„
        Extract file tree structure from initial_plan.txt content

        Args:
            plan_content: Content of the initial_plan.txt file

        Returns:
            Extracted file tree structure as string
        """
        # æŸ¥æ‰¾æ–‡ä»¶ç»“æ„éƒ¨åˆ†ï¼Œç‰¹åˆ«æ˜¯"## File Structure"æ ¼å¼
        file_structure_pattern = r"## File Structure[^\n]*\n```[^\n]*\n(.*?)\n```"

        match = re.search(file_structure_pattern, plan_content, re.DOTALL)
        if match:
            file_tree = match.group(1).strip()
            lines = file_tree.split("\n")

            # æ¸…ç†æ ‘ç»“æ„ - ç§»é™¤ç©ºè¡Œå’Œä¸å±äºç»“æ„çš„æ³¨é‡Š
            cleaned_lines = []
            for line in lines:
                # ä¿ç•™æ ‘ç»“æ„çš„è¡Œ
                if line.strip() and (
                    any(char in line for char in ["â”œâ”€â”€", "â””â”€â”€", "â”‚"])
                    or line.strip().endswith("/")
                    or "." in line.split("/")[-1]  # æœ‰æ–‡ä»¶æ‰©å±•å
                    or line.strip().endswith(".py")
                    or line.strip().endswith(".txt")
                    or line.strip().endswith(".md")
                    or line.strip().endswith(".yaml")
                ):
                    cleaned_lines.append(line)

            if len(cleaned_lines) >= 5:
                file_tree = "\n".join(cleaned_lines)
                self.logger.info(
                    f"ğŸ“Š ä»## File Structureéƒ¨åˆ†æå–æ–‡ä»¶æ ‘ç»“æ„ ({len(cleaned_lines)} lines)"
                )
                return file_tree

        # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾åŒ…å«é¡¹ç›®ç»“æ„çš„ä»»ä½•ä»£ç å—
        code_block_patterns = [
            r"```[^\n]*\n(rice_framework/.*?(?:â”œâ”€â”€|â””â”€â”€).*?)\n```",
            r"```[^\n]*\n(project/.*?(?:â”œâ”€â”€|â””â”€â”€).*?)\n```",
            r"```[^\n]*\n(src/.*?(?:â”œâ”€â”€|â””â”€â”€).*?)\n```",
            r"```[^\n]*\n(.*?(?:â”œâ”€â”€|â””â”€â”€).*?(?:\.py|\.txt|\.md|\.yaml).*?)\n```",
        ]

        for pattern in code_block_patterns:
            match = re.search(pattern, plan_content, re.DOTALL)
            if match:
                file_tree = match.group(1).strip()
                lines = [line for line in file_tree.split("\n") if line.strip()]
                if len(lines) >= 5:
                    self.logger.info(
                        f"ğŸ“Š ä»ä»£ç å—ä¸­æå–æ–‡ä»¶æ ‘ç»“æ„ ({len(lines)} lines)"
                    )
                    return file_tree

        # æœ€ç»ˆå¤‡ç”¨æ–¹æ¡ˆï¼šä»æ–‡ä»¶æåŠä¸­æå–æ–‡ä»¶è·¯å¾„å¹¶åˆ›å»ºåŸºæœ¬ç»“æ„
        self.logger.warning("âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†æ–‡ä»¶æ ‘ï¼Œå°è¯•ä»æ–‡ä»¶æåŠä¸­æå–...")

        # åœ¨æ•´ä¸ªæ–‡æ¡£ä¸­æŸ¥æ‰¾åå¼•å·ä¸­çš„æ–‡ä»¶è·¯å¾„
        file_mentions = re.findall(
            r"`([^`]*(?:\.py|\.txt|\.md|\.yaml|\.yml)[^`]*)`", plan_content
        )

        if file_mentions:
            # å°†æ–‡ä»¶ç»„ç»‡æˆç›®å½•ç»“æ„
            dirs = set()
            files_by_dir = {}

            for file_path in file_mentions:
                file_path = file_path.strip()
                if "/" in file_path:
                    dir_path = "/".join(file_path.split("/")[:-1])
                    filename = file_path.split("/")[-1]
                    dirs.add(dir_path)
                    if dir_path not in files_by_dir:
                        files_by_dir[dir_path] = []
                    files_by_dir[dir_path].append(filename)
                else:
                    if "root" not in files_by_dir:
                        files_by_dir["root"] = []
                    files_by_dir["root"].append(file_path)

            # åˆ›å»ºæ ‘ç»“æ„
            structure_lines = []

            # ç¡®å®šæ ¹ç›®å½•åç§°
            root_name = (
                "rice_framework"
                if any("rice" in f for f in file_mentions)
                else "project"
            )
            structure_lines.append(f"{root_name}/")

            # æ·»åŠ ç›®å½•å’Œæ–‡ä»¶
            sorted_dirs = sorted(dirs) if dirs else []
            for i, dir_path in enumerate(sorted_dirs):
                is_last_dir = i == len(sorted_dirs) - 1
                prefix = "â””â”€â”€" if is_last_dir else "â”œâ”€â”€"
                structure_lines.append(f"{prefix} {dir_path}/")

                if dir_path in files_by_dir:
                    files = sorted(files_by_dir[dir_path])
                    for j, filename in enumerate(files):
                        is_last_file = j == len(files) - 1
                        if is_last_dir:
                            file_prefix = "    â””â”€â”€" if is_last_file else "    â”œâ”€â”€"
                        else:
                            file_prefix = "â”‚   â””â”€â”€" if is_last_file else "â”‚   â”œâ”€â”€"
                        structure_lines.append(f"{file_prefix} {filename}")

            # æ·»åŠ æ ¹æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
            if "root" in files_by_dir:
                root_files = sorted(files_by_dir["root"])
                for i, filename in enumerate(root_files):
                    is_last = (i == len(root_files) - 1) and not sorted_dirs
                    prefix = "â””â”€â”€" if is_last else "â”œâ”€â”€"
                    structure_lines.append(f"{prefix} {filename}")

            if len(structure_lines) >= 3:
                file_tree = "\n".join(structure_lines)
                self.logger.info(
                    f"ğŸ“Š ä»æ–‡ä»¶æåŠç”Ÿæˆæ–‡ä»¶æ ‘ ({len(structure_lines)} lines)"
                )
                return file_tree

        # å¦‚æœæœªæ‰¾åˆ°æ–‡ä»¶æ ‘ï¼Œè¿”å›None
        self.logger.warning("âš ï¸ åœ¨åˆå§‹è®¡åˆ’ä¸­æœªæ‰¾åˆ°æ–‡ä»¶æ ‘ç»“æ„")
        return None

    def load_target_structure_from_plan(self, plan_path: str) -> str:
        """
        ä»initial_plan.txtåŠ è½½ç›®æ ‡ç»“æ„å¹¶æå–æ–‡ä»¶æ ‘
        Load target structure from initial_plan.txt and extract file tree

        Args:
            plan_path: Path to initial_plan.txt file

        Returns:
            Extracted file tree structure
        """
        try:
            # åŠ è½½å®Œæ•´çš„è®¡åˆ’å†…å®¹
            with open(plan_path, "r", encoding="utf-8") as f:
                plan_content = f.read()

            self.logger.info(f"ğŸ“„ å·²åŠ è½½åˆå§‹è®¡åˆ’ ({len(plan_content)} characters)")

            # æå–æ–‡ä»¶æ ‘ç»“æ„
            file_tree = self.extract_file_tree_from_plan(plan_content)

            if file_tree:
                self.logger.info("âœ… æˆåŠŸä»åˆå§‹è®¡åˆ’ä¸­æå–æ–‡ä»¶æ ‘")
                self.logger.info("ğŸ“‹ æå–ç»“æ„é¢„è§ˆ:")
                # æ˜¾ç¤ºæå–æ ‘çš„å‰å‡ è¡Œ
                preview_lines = file_tree.split("\n")[:8]
                for line in preview_lines:
                    self.logger.info(f"   {line}")
                if len(file_tree.split("\n")) > 8:
                    self.logger.info(f"   ... è¿˜æœ‰ {len(file_tree.split('\n')) - 8} è¡Œ")
                return file_tree
            else:
                self.logger.warning("âš ï¸ æ— æ³•ä»åˆå§‹è®¡åˆ’ä¸­æå–æ–‡ä»¶æ ‘")
                self.logger.info("ğŸ”„ å›é€€åˆ°é»˜è®¤ç›®æ ‡ç»“æ„")
                return self.get_default_target_structure()

        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½åˆå§‹è®¡åˆ’æ–‡ä»¶å¤±è´¥ {plan_path}: {e}")
            self.logger.info("ğŸ”„ å›é€€åˆ°é»˜è®¤ç›®æ ‡ç»“æ„")
            return self.get_default_target_structure()

    def get_default_target_structure(self) -> str:
        """è·å–é»˜è®¤ç›®æ ‡ç»“æ„"""
        return """
project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ gcn.py        # GCN encoder
â”‚   â”‚   â”œâ”€â”€ diffusion.py  # forward/reverse processes
â”‚   â”‚   â”œâ”€â”€ denoiser.py   # denoising MLP
â”‚   â”‚   â””â”€â”€ fusion.py     # fusion combiner
â”‚   â”œâ”€â”€ models/           # model wrapper classes
â”‚   â”‚   â””â”€â”€ recdiff.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ data.py       # loading & preprocessing
â”‚   â”‚   â”œâ”€â”€ predictor.py  # scoring functions
â”‚   â”‚   â”œâ”€â”€ loss.py       # loss functions
â”‚   â”‚   â”œâ”€â”€ metrics.py    # NDCG, Recall etc.
â”‚   â”‚   â””â”€â”€ sched.py      # beta/alpha schedule utils
â”‚   â””â”€â”€ configs/
â”‚       â””â”€â”€ default.yaml  # hyperparameters, paths
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_gcn.py
â”‚   â”œâ”€â”€ test_diffusion.py
â”‚   â”œâ”€â”€ test_denoiser.py
â”‚   â”œâ”€â”€ test_loss.py
â”‚   â””â”€â”€ test_pipeline.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ api_reference.md
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_experiment.py
â”‚   â””â”€â”€ notebooks/
â”‚       â””â”€â”€ analysis.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup.py
"""

    def load_or_create_indexer_config(self, paper_dir: str) -> Dict[str, Any]:
        """
        åŠ è½½æˆ–åˆ›å»ºç´¢å¼•å™¨é…ç½®
        Load or create indexer configuration

        Args:
            paper_dir: è®ºæ–‡ç›®å½•è·¯å¾„

        Returns:
            é…ç½®å­—å…¸
        """
        # å°è¯•åŠ è½½ç°æœ‰çš„é…ç½®æ–‡ä»¶
        config_path = Path(__file__).parent.parent / "tools" / "indexer_config.yaml"

        try:
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)

                # æ›´æ–°è·¯å¾„é…ç½®ä¸ºå½“å‰è®ºæ–‡ç›®å½•
                if "paths" not in config:
                    config["paths"] = {}
                config["paths"]["code_base_path"] = os.path.join(paper_dir, "code_base")
                config["paths"]["output_dir"] = os.path.join(paper_dir, "indexes")

                # è°ƒæ•´æ€§èƒ½è®¾ç½®ä»¥é€‚åº”å·¥ä½œæµ
                if "performance" in config:
                    config["performance"]["enable_concurrent_analysis"] = (
                        False  # ç¦ç”¨å¹¶å‘ä»¥é¿å…APIé™åˆ¶
                    )
                if "debug" in config:
                    config["debug"]["verbose_output"] = True  # å¯ç”¨è¯¦ç»†è¾“å‡º
                if "llm" in config:
                    config["llm"]["request_delay"] = 0.5  # å¢åŠ è¯·æ±‚é—´éš”

                self.logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                return config

        except Exception as e:
            self.logger.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

        # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        self.logger.info("ä½¿ç”¨é»˜è®¤é…ç½®")
        default_config = {
            "paths": {
                "code_base_path": os.path.join(paper_dir, "code_base"),
                "output_dir": os.path.join(paper_dir, "indexes"),
            },
            "llm": {
                "model_provider": "anthropic",
                "max_tokens": 4000,
                "temperature": 0.3,
                "request_delay": 0.5,  # å¢åŠ è¯·æ±‚é—´éš”
                "max_retries": 3,
                "retry_delay": 1.0,
            },
            "file_analysis": {
                "max_file_size": 1048576,  # 1MB
                "max_content_length": 3000,
                "supported_extensions": [
                    ".py",
                    ".js",
                    ".ts",
                    ".java",
                    ".cpp",
                    ".c",
                    ".h",
                    ".hpp",
                    ".cs",
                    ".php",
                    ".rb",
                    ".go",
                    ".rs",
                    ".scala",
                    ".kt",
                    ".yaml",
                    ".yml",
                    ".json",
                    ".xml",
                    ".toml",
                    ".md",
                    ".txt",
                ],
                "skip_directories": [
                    "__pycache__",
                    "node_modules",
                    "target",
                    "build",
                    "dist",
                    "venv",
                    "env",
                    ".git",
                    ".svn",
                    "data",
                    "datasets",
                ],
            },
            "relationships": {
                "min_confidence_score": 0.3,
                "high_confidence_threshold": 0.7,
                "relationship_types": {
                    "direct_match": 1.0,
                    "partial_match": 0.8,
                    "reference": 0.6,
                    "utility": 0.4,
                },
            },
            "performance": {
                "enable_concurrent_analysis": False,  # ç¦ç”¨å¹¶å‘ä»¥é¿å…APIé™åˆ¶
                "max_concurrent_files": 3,
                "enable_content_caching": True,
                "max_cache_size": 100,
            },
            "debug": {
                "verbose_output": True,
                "save_raw_responses": False,
                "mock_llm_responses": False,
            },
            "output": {
                "generate_summary": True,
                "generate_statistics": True,
                "include_metadata": True,
                "json_indent": 2,
            },
            "logging": {"level": "INFO", "log_to_file": False},
        }

        return default_config

    async def run_indexing_workflow(
        self,
        paper_dir: str,
        initial_plan_path: Optional[str] = None,
        config_path: str = "mcp_agent.secrets.yaml",
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„ä»£ç ç´¢å¼•å·¥ä½œæµ
        Run the complete code indexing workflow

        Args:
            paper_dir: è®ºæ–‡ç›®å½•è·¯å¾„
            initial_plan_path: åˆå§‹è®¡åˆ’æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            config_path: APIé…ç½®æ–‡ä»¶è·¯å¾„

        Returns:
            ç´¢å¼•ç»“æœå­—å…¸
        """
        try:
            self.logger.info("ğŸš€ å¼€å§‹ä»£ç åº“ç´¢å¼•å·¥ä½œæµ...")

            # æ­¥éª¤1ï¼šç¡®å®šåˆå§‹è®¡åˆ’æ–‡ä»¶è·¯å¾„
            if not initial_plan_path:
                initial_plan_path = os.path.join(paper_dir, "initial_plan.txt")

            # æ­¥éª¤2ï¼šåŠ è½½ç›®æ ‡ç»“æ„
            if os.path.exists(initial_plan_path):
                self.logger.info(f"ğŸ“ ä» {initial_plan_path} åŠ è½½ç›®æ ‡ç»“æ„")
                target_structure = self.load_target_structure_from_plan(
                    initial_plan_path
                )
            else:
                self.logger.warning(f"âš ï¸ åˆå§‹è®¡åˆ’æ–‡ä»¶ä¸å­˜åœ¨: {initial_plan_path}")
                self.logger.info("ğŸ“ ä½¿ç”¨é»˜è®¤ç›®æ ‡ç»“æ„")
                target_structure = self.get_default_target_structure()

            # æ­¥éª¤3ï¼šæ£€æŸ¥ä»£ç åº“è·¯å¾„
            code_base_path = os.path.join(paper_dir, "code_base")
            if not os.path.exists(code_base_path):
                self.logger.error(f"âŒ ä»£ç åº“è·¯å¾„ä¸å­˜åœ¨: {code_base_path}")
                return {
                    "status": "error",
                    "message": f"Code base path does not exist: {code_base_path}",
                    "output_files": {},
                }

            # æ­¥éª¤4ï¼šåˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = os.path.join(paper_dir, "indexes")
            os.makedirs(output_dir, exist_ok=True)

            # æ­¥éª¤5ï¼šåŠ è½½é…ç½®
            indexer_config = self.load_or_create_indexer_config(paper_dir)

            self.logger.info(f"ğŸ“ ä»£ç åº“è·¯å¾„: {code_base_path}")
            self.logger.info(f"ğŸ“¤ è¾“å‡ºç›®å½•: {output_dir}")

            # æ­¥éª¤6ï¼šåˆ›å»ºä»£ç ç´¢å¼•å™¨
            self.indexer = CodeIndexer(
                code_base_path=code_base_path,
                target_structure=target_structure,
                output_dir=output_dir,
                config_path=config_path,
                enable_pre_filtering=True,
            )

            # åº”ç”¨é…ç½®è®¾ç½® / Apply configuration settings
            self.indexer.indexer_config = indexer_config

            # ç›´æ¥è®¾ç½®é…ç½®å±æ€§åˆ°ç´¢å¼•å™¨ / Directly set configuration attributes to indexer
            if "file_analysis" in indexer_config:
                file_config = indexer_config["file_analysis"]
                self.indexer.supported_extensions = set(
                    file_config.get(
                        "supported_extensions", self.indexer.supported_extensions
                    )
                )
                self.indexer.skip_directories = set(
                    file_config.get("skip_directories", self.indexer.skip_directories)
                )
                self.indexer.max_file_size = file_config.get(
                    "max_file_size", self.indexer.max_file_size
                )
                self.indexer.max_content_length = file_config.get(
                    "max_content_length", self.indexer.max_content_length
                )

            if "llm" in indexer_config:
                llm_config = indexer_config["llm"]
                self.indexer.model_provider = llm_config.get(
                    "model_provider", self.indexer.model_provider
                )
                self.indexer.llm_max_tokens = llm_config.get(
                    "max_tokens", self.indexer.llm_max_tokens
                )
                self.indexer.llm_temperature = llm_config.get(
                    "temperature", self.indexer.llm_temperature
                )
                self.indexer.request_delay = llm_config.get(
                    "request_delay", self.indexer.request_delay
                )
                self.indexer.max_retries = llm_config.get(
                    "max_retries", self.indexer.max_retries
                )
                self.indexer.retry_delay = llm_config.get(
                    "retry_delay", self.indexer.retry_delay
                )

            if "relationships" in indexer_config:
                rel_config = indexer_config["relationships"]
                self.indexer.min_confidence_score = rel_config.get(
                    "min_confidence_score", self.indexer.min_confidence_score
                )
                self.indexer.high_confidence_threshold = rel_config.get(
                    "high_confidence_threshold", self.indexer.high_confidence_threshold
                )
                self.indexer.relationship_types = rel_config.get(
                    "relationship_types", self.indexer.relationship_types
                )

            if "performance" in indexer_config:
                perf_config = indexer_config["performance"]
                self.indexer.enable_concurrent_analysis = perf_config.get(
                    "enable_concurrent_analysis",
                    self.indexer.enable_concurrent_analysis,
                )
                self.indexer.max_concurrent_files = perf_config.get(
                    "max_concurrent_files", self.indexer.max_concurrent_files
                )
                self.indexer.enable_content_caching = perf_config.get(
                    "enable_content_caching", self.indexer.enable_content_caching
                )
                self.indexer.max_cache_size = perf_config.get(
                    "max_cache_size", self.indexer.max_cache_size
                )

            if "debug" in indexer_config:
                debug_config = indexer_config["debug"]
                self.indexer.verbose_output = debug_config.get(
                    "verbose_output", self.indexer.verbose_output
                )
                self.indexer.save_raw_responses = debug_config.get(
                    "save_raw_responses", self.indexer.save_raw_responses
                )
                self.indexer.mock_llm_responses = debug_config.get(
                    "mock_llm_responses", self.indexer.mock_llm_responses
                )

            if "output" in indexer_config:
                output_config = indexer_config["output"]
                self.indexer.generate_summary = output_config.get(
                    "generate_summary", self.indexer.generate_summary
                )
                self.indexer.generate_statistics = output_config.get(
                    "generate_statistics", self.indexer.generate_statistics
                )
                self.indexer.include_metadata = output_config.get(
                    "include_metadata", self.indexer.include_metadata
                )

            self.logger.info("ğŸ”§ ç´¢å¼•å™¨é…ç½®å®Œæˆ")
            self.logger.info(f"ğŸ¤– æ¨¡å‹æä¾›å•†: {self.indexer.model_provider}")
            self.logger.info(
                f"âš¡ å¹¶å‘åˆ†æ: {'å¯ç”¨' if self.indexer.enable_concurrent_analysis else 'ç¦ç”¨'}"
            )
            self.logger.info(
                f"ğŸ—„ï¸ å†…å®¹ç¼“å­˜: {'å¯ç”¨' if self.indexer.enable_content_caching else 'ç¦ç”¨'}"
            )
            self.logger.info(
                f"ğŸ” é¢„è¿‡æ»¤: {'å¯ç”¨' if self.indexer.enable_pre_filtering else 'ç¦ç”¨'}"
            )

            self.logger.info("=" * 60)
            self.logger.info("ğŸš€ å¼€å§‹ä»£ç ç´¢å¼•è¿‡ç¨‹...")

            # æ­¥éª¤7ï¼šæ„å»ºæ‰€æœ‰ç´¢å¼•
            output_files = await self.indexer.build_all_indexes()

            # æ­¥éª¤8ï¼šç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            if output_files:
                summary_report = self.indexer.generate_summary_report(output_files)

                self.logger.info("=" * 60)
                self.logger.info("âœ… ç´¢å¼•å®ŒæˆæˆåŠŸ!")
                self.logger.info(f"ğŸ“Š å¤„ç†äº† {len(output_files)} ä¸ªä»“åº“")
                self.logger.info("ğŸ“ ç”Ÿæˆçš„ç´¢å¼•æ–‡ä»¶:")
                for repo_name, file_path in output_files.items():
                    self.logger.info(f"   ğŸ“„ {repo_name}: {file_path}")
                self.logger.info(f"ğŸ“‹ æ‘˜è¦æŠ¥å‘Š: {summary_report}")

                # ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.indexer.generate_statistics:
                    self.logger.info("\nğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
                    total_relationships = 0
                    high_confidence_relationships = 0

                    for file_path in output_files.values():
                        try:
                            with open(file_path, "r", encoding="utf-8") as f:
                                index_data = json.load(f)
                                relationships = index_data.get("relationships", [])
                                total_relationships += len(relationships)
                                high_confidence_relationships += len(
                                    [
                                        r
                                        for r in relationships
                                        if r.get("confidence_score", 0)
                                        > self.indexer.high_confidence_threshold
                                    ]
                                )
                        except Exception as e:
                            self.logger.warning(
                                f"   âš ï¸ æ— æ³•ä» {file_path} åŠ è½½ç»Ÿè®¡: {e}"
                            )

                    self.logger.info(f"   ğŸ”— æ‰¾åˆ°çš„æ€»å…³ç³»æ•°: {total_relationships}")
                    self.logger.info(
                        f"   â­ é«˜ç½®ä¿¡åº¦å…³ç³»: {high_confidence_relationships}"
                    )
                    self.logger.info(
                        f"   ğŸ“Š æ¯ä¸ªä»“åº“çš„å¹³å‡å…³ç³»: {total_relationships / len(output_files) if output_files else 0:.1f}"
                    )

                self.logger.info("\nğŸ‰ ä»£ç ç´¢å¼•è¿‡ç¨‹æˆåŠŸå®Œæˆ!")

                return {
                    "status": "success",
                    "message": f"Successfully indexed {len(output_files)} repositories",
                    "output_files": output_files,
                    "summary_report": summary_report,
                    "statistics": {
                        "total_repositories": len(output_files),
                        "total_relationships": total_relationships,
                        "high_confidence_relationships": high_confidence_relationships,
                    }
                    if self.indexer.generate_statistics
                    else None,
                }
            else:
                self.logger.warning("âš ï¸ æœªç”Ÿæˆç´¢å¼•æ–‡ä»¶")
                return {
                    "status": "warning",
                    "message": "No index files were generated",
                    "output_files": {},
                }

        except Exception as e:
            self.logger.error(f"âŒ ç´¢å¼•å·¥ä½œæµå¤±è´¥: {e}")
            # å¦‚æœæœ‰è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œè®°å½•ä¸‹æ¥
            import traceback

            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return {"status": "error", "message": str(e), "output_files": {}}

    def print_banner(self):
        """æ‰“å°åº”ç”¨æ¨ªå¹…"""
        banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ” Codebase Index Workflow v1.0                   â•‘
â•‘              Intelligent Code Relationship Analysis Tool              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“ åˆ†æç°æœ‰ä»£ç åº“ / Analyzes existing codebases                     â•‘
â•‘  ğŸ”— ä¸ç›®æ ‡ç»“æ„å»ºç«‹æ™ºèƒ½å…³ç³» / Builds intelligent relationships        â•‘
â•‘  ğŸ¤– ç”±LLMåˆ†æé©±åŠ¨ / Powered by LLM analysis                          â•‘
â•‘  ğŸ“Š ç”Ÿæˆè¯¦ç»†çš„JSONç´¢å¼• / Generates detailed JSON indexes             â•‘
â•‘  ğŸ¯ ä¸ºä»£ç å¤ç°æä¾›å‚è€ƒ / Provides reference for code reproduction    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print(banner)


# ä¾¿æ·å‡½æ•°ï¼Œç”¨äºç›´æ¥è°ƒç”¨å·¥ä½œæµ
async def run_codebase_indexing(
    paper_dir: str,
    initial_plan_path: Optional[str] = None,
    config_path: str = "mcp_agent.secrets.yaml",
    logger=None,
) -> Dict[str, Any]:
    """
    è¿è¡Œä»£ç åº“ç´¢å¼•çš„ä¾¿æ·å‡½æ•°
    Convenience function to run codebase indexing

    Args:
        paper_dir: è®ºæ–‡ç›®å½•è·¯å¾„
        initial_plan_path: åˆå§‹è®¡åˆ’æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        config_path: APIé…ç½®æ–‡ä»¶è·¯å¾„
        logger: æ—¥å¿—è®°å½•å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰

    Returns:
        ç´¢å¼•ç»“æœå­—å…¸
    """
    workflow = CodebaseIndexWorkflow(logger=logger)
    workflow.print_banner()

    return await workflow.run_indexing_workflow(
        paper_dir=paper_dir,
        initial_plan_path=initial_plan_path,
        config_path=config_path,
    )


# ç”¨äºæµ‹è¯•çš„ä¸»å‡½æ•°
async def main():
    """ä¸»å‡½æ•°ç”¨äºæµ‹è¯•å·¥ä½œæµ"""
    import logging

    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    # æµ‹è¯•å‚æ•°
    paper_dir = "./deepcode_lab/papers/1"
    initial_plan_path = os.path.join(paper_dir, "initial_plan.txt")

    # è¿è¡Œå·¥ä½œæµ
    result = await run_codebase_indexing(
        paper_dir=paper_dir, initial_plan_path=initial_plan_path, logger=logger
    )

    logger.info(f"ç´¢å¼•ç»“æœ: {result}")


if __name__ == "__main__":
    asyncio.run(main())
